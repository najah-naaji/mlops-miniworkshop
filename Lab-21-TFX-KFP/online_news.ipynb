{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QNxEZK2pPA1P"
   },
   "source": [
    "##### Copyright &copy; 2019 The TensorFlow Authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7JwKPOmN2-15"
   },
   "outputs": [],
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "23R0Z9RojXYW"
   },
   "source": [
    "# Using the KFP Orchestrator for TFX\n",
    "\n",
    "The notebook demonstrates how to define, deploy and run the TFX pipeline that utilizes Google Cloud Platform's (GCP) Cloud AI Platfom (CAIP) services for processing and Kubeflow Pipelines (KFP) for orchestrations. In this example, KFP is running on Google Kubernetes (GKE) Engine and utilizes Cloud SQL for ML Metadata and Google Cloud Storage (GCS) for artifact store."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N-ePgV0Lj68Q"
   },
   "source": [
    "### Import packages\n",
    "We import necessary packages, required to define, compile and deploy the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YIqpWK9efviJ"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "import urllib\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import tfx\n",
    "import kfp\n",
    "from tfx.components.evaluator.component import Evaluator\n",
    "from tfx.components.example_gen.csv_example_gen.component import CsvExampleGen\n",
    "from tfx.components.example_validator.component import ExampleValidator\n",
    "from tfx.components.model_validator.component import ModelValidator\n",
    "from tfx.components.pusher.component import Pusher\n",
    "from tfx.components.schema_gen.component import SchemaGen\n",
    "from tfx.components.statistics_gen.component import StatisticsGen\n",
    "from tfx.components.trainer.component import Trainer\n",
    "from tfx.components.transform.component import Transform\n",
    "from tfx.proto import evaluator_pb2\n",
    "from tfx.proto import pusher_pb2\n",
    "from tfx.proto import trainer_pb2\n",
    "\n",
    "from tfx.orchestration import metadata\n",
    "from tfx.orchestration import pipeline\n",
    "from tfx.orchestration.kubeflow import kubeflow_dag_runner\n",
    "from tfx.orchestration.kubeflow.proto import kubeflow_pb2\n",
    "\n",
    "from tfx.extensions.google_cloud_ai_platform.trainer import executor as ai_platform_trainer_executor  # pylint: disable=g-import-not-at-top\n",
    "from tfx.extensions.google_cloud_ai_platform.pusher import executor as ai_platform_pusher_executor  # pylint: disable=g-import-not-at-top\n",
    "\n",
    "from tfx.utils.dsl_utils import external_input\n",
    "\n",
    "from tfx.components.base import executor_spec\n",
    "\n",
    "from typing import Dict, List, Text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KfS6otsARvJC"
   },
   "source": [
    "Check the versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XZY7Pnoxmoe8"
   },
   "outputs": [],
   "source": [
    "print('TensorFlow version: {}'.format(tf.__version__))\n",
    "print('TFX version: {}'.format(tfx.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n2cMMAbSkGfX"
   },
   "source": [
    "### Download example data\n",
    "We download the sample dataset for use in our TFX pipeline.  We're working with a variant of the [Online News Popularity](https://archive.ics.uci.edu/ml/datasets/online+news+popularity) dataset, which summarizes a heterogeneous set of features about articles published by Mashable in a period of two years. The goal is to predict how popular the article will be on social networks. Specifically, in the original dataset the objective was to predict the number of times each article will be shared on social networks. In this variant, the goal is to predict the article's popularity percentile. For example, if the model predicts a score of 0.7, then it means it expects the article to be shared more than 70% of all articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BywX6OUEhAqn"
   },
   "outputs": [],
   "source": [
    "# Download the example data.\n",
    "DATA_PATH = 'https://raw.githubusercontent.com/ageron/open-datasets/master/' \\\n",
    "   'online_news_popularity_for_course/online_news_popularity_for_course.csv'\n",
    "_data_root = tempfile.mkdtemp(prefix='tfx-data')\n",
    "_data_filepath = os.path.join(_data_root, \"data.csv\")\n",
    "urllib.request.urlretrieve(DATA_PATH, _data_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pXu5IR6dSDwJ"
   },
   "source": [
    "Take a quick look at the CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Hqn4wST2Bex5"
   },
   "outputs": [],
   "source": [
    "!head {_data_filepath}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload the dataset to GCS\n",
    "\n",
    "The pipeline will execute on GKE running on GCP and will utilize Cloud services, including Dataflow, AI Platform Training, and AI Platform Prediction. To make the dataset accessible to the pipeline and the services you need to upload it to GCS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_artifact_store_bucket = 'tfw-demo-artifact-store'\n",
    "_gcs_data_root = 'gs://{}/{}/'.format(_artifact_store_bucket, 'data')\n",
    "\n",
    "!gsutil cp $_data_filepath $_gcs_data_root"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ufJKQ6OvkJlY"
   },
   "source": [
    "### Create the Transform and Trainer routines\n",
    "\n",
    "Create a file containing routines utilized by the Transform and Trainer components. To simplify the demo configuration we define both Transform and Trainer routines in a single file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ad5JLpKbf6sN"
   },
   "outputs": [],
   "source": [
    "_module_file = 'transform_train.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_GpU9-JNXw-_"
   },
   "outputs": [],
   "source": [
    "%%writefile {_module_file}\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_transform as tft\n",
    "import tensorflow_model_analysis as tfma\n",
    "from tensorflow_transform.tf_metadata import schema_utils\n",
    "\n",
    "DENSE_FLOAT_FEATURE_KEYS = [\n",
    "    \"timedelta\", \"n_tokens_title\", \"n_tokens_content\",\n",
    "    \"n_unique_tokens\", \"n_non_stop_words\", \"n_non_stop_unique_tokens\",\n",
    "    \"n_hrefs\", \"n_self_hrefs\", \"n_imgs\", \"n_videos\", \"average_token_length\",\n",
    "    \"n_keywords\", \"kw_min_min\", \"kw_max_min\", \"kw_avg_min\", \"kw_min_max\",\n",
    "    \"kw_max_max\", \"kw_avg_max\", \"kw_min_avg\", \"kw_max_avg\", \"kw_avg_avg\",\n",
    "    \"self_reference_min_shares\", \"self_reference_max_shares\",\n",
    "    \"self_reference_avg_shares\", \"is_weekend\", \"global_subjectivity\",\n",
    "    \"global_sentiment_polarity\", \"global_rate_positive_words\",\n",
    "    \"global_rate_negative_words\", \"rate_positive_words\", \"rate_negative_words\",\n",
    "    \"avg_positive_polarity\", \"min_positive_polarity\", \"max_positive_polarity\",\n",
    "    \"avg_negative_polarity\", \"min_negative_polarity\", \"max_negative_polarity\",\n",
    "    \"title_subjectivity\", \"title_sentiment_polarity\", \"abs_title_subjectivity\",\n",
    "    \"abs_title_sentiment_polarity\"]\n",
    "\n",
    "VOCAB_FEATURE_KEYS = [\"data_channel\"]\n",
    "\n",
    "BUCKET_FEATURE_KEYS = [\"LDA_00\", \"LDA_01\", \"LDA_02\", \"LDA_03\", \"LDA_04\"]\n",
    "\n",
    "CATEGORICAL_FEATURE_KEYS = [\"weekday\"]\n",
    "\n",
    "# Categorical features are assumed to each have a maximum value in the dataset.\n",
    "MAX_CATEGORICAL_FEATURE_VALUES = [6]\n",
    "\n",
    "#UNUSED: date, slug\n",
    "\n",
    "LABEL_KEY = \"n_shares_percentile\"\n",
    "VOCAB_SIZE = 10\n",
    "OOV_SIZE = 5\n",
    "FEATURE_BUCKET_COUNT = 10\n",
    "\n",
    "def transformed_name(key):\n",
    "  return key + '_xf'\n",
    "\n",
    "def preprocessing_fn(inputs):\n",
    "  \"\"\"tf.transform's callback function for preprocessing inputs.\n",
    "\n",
    "  Args:\n",
    "    inputs: map from feature keys to raw not-yet-transformed features.\n",
    "\n",
    "  Returns:\n",
    "    Map from string feature key to transformed feature operations.\n",
    "  \"\"\"\n",
    "  outputs = {}\n",
    "  for key in DENSE_FLOAT_FEATURE_KEYS:\n",
    "    # Preserve this feature as a dense float, setting nan's to the mean.\n",
    "    outputs[transformed_name(key)] = tft.scale_to_z_score(\n",
    "        _fill_in_missing(inputs[key]))\n",
    "\n",
    "  for key in VOCAB_FEATURE_KEYS:\n",
    "    # Build a vocabulary for this feature.\n",
    "    outputs[transformed_name(key)] = tft.compute_and_apply_vocabulary(\n",
    "        _fill_in_missing(inputs[key]),\n",
    "        top_k=VOCAB_SIZE,\n",
    "        num_oov_buckets=OOV_SIZE)\n",
    "\n",
    "  for key in BUCKET_FEATURE_KEYS:\n",
    "    outputs[transformed_name(key)] = tft.bucketize(\n",
    "        _fill_in_missing(inputs[key]), FEATURE_BUCKET_COUNT,\n",
    "        always_return_num_quantiles=False)\n",
    "\n",
    "  for key in CATEGORICAL_FEATURE_KEYS:\n",
    "    outputs[transformed_name(key)] = _fill_in_missing(inputs[key])\n",
    "\n",
    "  # How popular is this article?\n",
    "  outputs[transformed_name(LABEL_KEY)] = _fill_in_missing(inputs[LABEL_KEY])\n",
    "\n",
    "  return outputs\n",
    "\n",
    "def _fill_in_missing(x):\n",
    "  \"\"\"Replace missing values in a SparseTensor.\n",
    "\n",
    "  Fills in missing values of `x` with '' or 0, and converts to a dense tensor.\n",
    "\n",
    "  Args:\n",
    "    x: A `SparseTensor` of rank 2.  Its dense shape should have size at most 1\n",
    "      in the second dimension.\n",
    "\n",
    "  Returns:\n",
    "    A rank 1 tensor where missing values of `x` have been filled in.\n",
    "  \"\"\"\n",
    "  default_value = '' if x.dtype == tf.string else 0\n",
    "  return tf.squeeze(\n",
    "      tf.sparse.to_dense(\n",
    "          tf.SparseTensor(x.indices, x.values, [x.dense_shape[0], 1]),\n",
    "          default_value),\n",
    "      axis=1)\n",
    "\n",
    "def transformed_names(keys):\n",
    "  return [transformed_name(key) for key in keys]\n",
    "\n",
    "\n",
    "# Tf.Transform considers these features as \"raw\"\n",
    "def _get_raw_feature_spec(schema):\n",
    "  return schema_utils.schema_as_feature_spec(schema).feature_spec\n",
    "\n",
    "\n",
    "def _gzip_reader_fn(filenames):\n",
    "  \"\"\"Small utility returning a record reader that can read gzip'ed files.\"\"\"\n",
    "  return tf.data.TFRecordDataset(\n",
    "      filenames,\n",
    "      compression_type='GZIP')\n",
    "\n",
    "\n",
    "def _build_estimator(config, hidden_units=None, warm_start_from=None):\n",
    "  \"\"\"Build an estimator for predicting the popularity of online news articles\n",
    "\n",
    "  Args:\n",
    "    config: tf.estimator.RunConfig defining the runtime environment for the\n",
    "      estimator (including model_dir).\n",
    "    hidden_units: [int], the layer sizes of the DNN (input layer first)\n",
    "    warm_start_from: Optional directory to warm start from.\n",
    "\n",
    "  Returns:\n",
    "    A dict of the following:\n",
    "      - estimator: The estimator that will be used for training and eval.\n",
    "      - train_spec: Spec for training.\n",
    "      - eval_spec: Spec for eval.\n",
    "      - eval_input_receiver_fn: Input function for eval.\n",
    "  \"\"\"\n",
    "  real_valued_columns = [\n",
    "      tf.feature_column.numeric_column(key, shape=())\n",
    "      for key in transformed_names(DENSE_FLOAT_FEATURE_KEYS)\n",
    "  ]\n",
    "  categorical_columns = [\n",
    "      tf.feature_column.categorical_column_with_identity(\n",
    "          key, num_buckets=VOCAB_SIZE + OOV_SIZE, default_value=0)\n",
    "      for key in transformed_names(VOCAB_FEATURE_KEYS)\n",
    "  ]\n",
    "  categorical_columns += [\n",
    "      tf.feature_column.categorical_column_with_identity(\n",
    "          key, num_buckets=FEATURE_BUCKET_COUNT, default_value=0)\n",
    "      for key in transformed_names(BUCKET_FEATURE_KEYS)\n",
    "  ]\n",
    "  categorical_columns += [\n",
    "      tf.feature_column.categorical_column_with_identity(\n",
    "          key,\n",
    "          num_buckets=num_buckets,\n",
    "          default_value=0) for key, num_buckets in zip(\n",
    "              transformed_names(CATEGORICAL_FEATURE_KEYS),\n",
    "              MAX_CATEGORICAL_FEATURE_VALUES)\n",
    "  ]\n",
    "  return tf.estimator.DNNLinearCombinedRegressor(\n",
    "      config=config,\n",
    "      linear_feature_columns=categorical_columns,\n",
    "      dnn_feature_columns=real_valued_columns,\n",
    "      dnn_hidden_units=hidden_units or [100, 70, 50, 25],\n",
    "      warm_start_from=warm_start_from)\n",
    "\n",
    "\n",
    "def _example_serving_receiver_fn(tf_transform_output, schema):\n",
    "  \"\"\"Build the serving in inputs.\n",
    "\n",
    "  Args:\n",
    "    tf_transform_output: A TFTransformOutput.\n",
    "    schema: the schema of the input data.\n",
    "\n",
    "  Returns:\n",
    "    Tensorflow graph which parses examples, applying tf-transform to them.\n",
    "  \"\"\"\n",
    "  raw_feature_spec = _get_raw_feature_spec(schema)\n",
    "  raw_feature_spec.pop(LABEL_KEY)\n",
    "\n",
    "  raw_input_fn = tf.estimator.export.build_parsing_serving_input_receiver_fn(\n",
    "      raw_feature_spec, default_batch_size=None)\n",
    "  serving_input_receiver = raw_input_fn()\n",
    "\n",
    "  transformed_features = tf_transform_output.transform_raw_features(\n",
    "      serving_input_receiver.features)\n",
    "\n",
    "  return tf.estimator.export.ServingInputReceiver(\n",
    "      transformed_features, serving_input_receiver.receiver_tensors)\n",
    "\n",
    "\n",
    "def _eval_input_receiver_fn(tf_transform_output, schema):\n",
    "  \"\"\"Build everything needed for the tf-model-analysis to run the model.\n",
    "\n",
    "  Args:\n",
    "    tf_transform_output: A TFTransformOutput.\n",
    "    schema: the schema of the input data.\n",
    "\n",
    "  Returns:\n",
    "    EvalInputReceiver function, which contains:\n",
    "      - Tensorflow graph which parses raw untransformed features, applies the\n",
    "        tf-transform preprocessing operators.\n",
    "      - Set of raw, untransformed features.\n",
    "      - Label against which predictions will be compared.\n",
    "  \"\"\"\n",
    "  # Notice that the inputs are raw features, not transformed features here.\n",
    "  raw_feature_spec = _get_raw_feature_spec(schema)\n",
    "\n",
    "  raw_input_fn = tf.estimator.export.build_parsing_serving_input_receiver_fn(\n",
    "      raw_feature_spec, default_batch_size=None)\n",
    "  serving_input_receiver = raw_input_fn()\n",
    "\n",
    "  features = serving_input_receiver.features.copy()\n",
    "  transformed_features = tf_transform_output.transform_raw_features(features)\n",
    "  \n",
    "  # NOTE: Model is driven by transformed features (since training works on the\n",
    "  # materialized output of TFT, but slicing will happen on raw features.\n",
    "  features.update(transformed_features)\n",
    "\n",
    "  return tfma.export.EvalInputReceiver(\n",
    "      features=features,\n",
    "      receiver_tensors=serving_input_receiver.receiver_tensors,\n",
    "      labels=transformed_features[transformed_name(LABEL_KEY)])\n",
    "\n",
    "\n",
    "def _input_fn(filenames, tf_transform_output, batch_size=200):\n",
    "  \"\"\"Generates features and labels for training or evaluation.\n",
    "\n",
    "  Args:\n",
    "    filenames: [str] list of CSV files to read data from.\n",
    "    tf_transform_output: A TFTransformOutput.\n",
    "    batch_size: int First dimension size of the Tensors returned by input_fn\n",
    "\n",
    "  Returns:\n",
    "    A (features, indices) tuple where features is a dictionary of\n",
    "      Tensors, and indices is a single Tensor of label indices.\n",
    "  \"\"\"\n",
    "  transformed_feature_spec = (\n",
    "      tf_transform_output.transformed_feature_spec().copy())\n",
    "\n",
    "  dataset = tf.data.experimental.make_batched_features_dataset(\n",
    "      filenames, batch_size, transformed_feature_spec, reader=_gzip_reader_fn)\n",
    "\n",
    "  transformed_features = dataset.make_one_shot_iterator().get_next()\n",
    "  # We pop the label because we do not want to use it as a feature while we're\n",
    "  # training.\n",
    "  return transformed_features, transformed_features.pop(\n",
    "      transformed_name(LABEL_KEY))\n",
    "\n",
    "\n",
    "# TFX will call this function\n",
    "def trainer_fn(hparams, schema):\n",
    "  \"\"\"Build the estimator using the high level API.\n",
    "  Args:\n",
    "    hparams: Holds hyperparameters used to train the model as name/value pairs.\n",
    "    schema: Holds the schema of the training examples.\n",
    "  Returns:\n",
    "    A dict of the following:\n",
    "      - estimator: The estimator that will be used for training and eval.\n",
    "      - train_spec: Spec for training.\n",
    "      - eval_spec: Spec for eval.\n",
    "      - eval_input_receiver_fn: Input function for eval.\n",
    "  \"\"\"\n",
    "  # Number of nodes in the first layer of the DNN\n",
    "  first_dnn_layer_size = 100\n",
    "  num_dnn_layers = 4\n",
    "  dnn_decay_factor = 0.7\n",
    "\n",
    "  train_batch_size = 40\n",
    "  eval_batch_size = 40\n",
    "\n",
    "  tf_transform_output = tft.TFTransformOutput(hparams.transform_output)\n",
    "\n",
    "  train_input_fn = lambda: _input_fn(\n",
    "      hparams.train_files,\n",
    "      tf_transform_output,\n",
    "      batch_size=train_batch_size)\n",
    "\n",
    "  eval_input_fn = lambda: _input_fn(\n",
    "      hparams.eval_files,\n",
    "      tf_transform_output,\n",
    "      batch_size=eval_batch_size)\n",
    "\n",
    "  train_spec = tf.estimator.TrainSpec(\n",
    "      train_input_fn,\n",
    "      max_steps=hparams.train_steps)\n",
    "\n",
    "  serving_receiver_fn = lambda: _example_serving_receiver_fn(\n",
    "      tf_transform_output, schema)\n",
    "\n",
    "  exporter = tf.estimator.FinalExporter('online-news', serving_receiver_fn)\n",
    "  eval_spec = tf.estimator.EvalSpec(\n",
    "      eval_input_fn,\n",
    "      steps=hparams.eval_steps,\n",
    "      exporters=[exporter],\n",
    "      name='online-news-eval')\n",
    "\n",
    "  run_config = tf.estimator.RunConfig(\n",
    "      save_checkpoints_steps=999, keep_checkpoint_max=1)\n",
    "\n",
    "  run_config = run_config.replace(model_dir=hparams.serving_model_dir)\n",
    "\n",
    "  estimator = _build_estimator(\n",
    "      # Construct layers sizes with exponetial decay\n",
    "      hidden_units=[\n",
    "          max(2, int(first_dnn_layer_size * dnn_decay_factor**i))\n",
    "          for i in range(num_dnn_layers)\n",
    "      ],\n",
    "      config=run_config,\n",
    "      warm_start_from=hparams.warm_start_from)\n",
    "\n",
    "  # Create an input receiver for TFMA processing\n",
    "  receiver_fn = lambda: _eval_input_receiver_fn(\n",
    "      tf_transform_output, schema)\n",
    "\n",
    "  return {\n",
    "      'estimator': estimator,\n",
    "      'train_spec': train_spec,\n",
    "      'eval_spec': eval_spec,\n",
    "      'eval_input_receiver_fn': receiver_fn\n",
    "  }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copy the module file to GCS\n",
    "The Transform and Trainer routines need access to the file at runtime. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_gcs_module_file = 'gs://{}/{}/{}'.format(_artifact_store_bucket, 'module', _module_file)\n",
    "\n",
    "!gsutil cp $_module_file $_gcs_module_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qAika7-6gLvI"
   },
   "source": [
    "## Define the Pipeline\n",
    "\n",
    "The next step is to create a function defining the workflow. As you examine the function, you will notice that the Trainer and Pusher components are configured to use AI Platform Training and AI Platform Prediction and other components are configured to utilize Dataflow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gNvMj9AWsmSt"
   },
   "outputs": [],
   "source": [
    "def _create_pipeline(\n",
    "    pipeline_name: Text, \n",
    "    pipeline_root: Text, \n",
    "    data_root: Text,\n",
    "    module_file: Text,\n",
    "    beam_pipeline_args: List[Text],\n",
    "    ai_platform_training_args: Dict[Text, Text],\n",
    "    ai_platform_serving_args: Dict[Text, Text]) -> pipeline.Pipeline:\n",
    "  \"\"\"Implements the online news pipeline with TFX.\"\"\"\n",
    "\n",
    "  examples = external_input(data_root)\n",
    "\n",
    "  # Brings data into the pipeline or otherwise joins/converts training data.\n",
    "  example_gen = CsvExampleGen(input_base=examples)\n",
    "\n",
    "  # Computes statistics over data for visualization and example validation.\n",
    "  statistics_gen = StatisticsGen(input_data=example_gen.outputs.examples)\n",
    "\n",
    "  # Generates schema based on statistics files.\n",
    "  infer_schema = SchemaGen(\n",
    "      stats=statistics_gen.outputs.output)\n",
    "\n",
    "  # Performs anomaly detection based on statistics and data schema.\n",
    "  validate_stats = ExampleValidator(\n",
    "      stats=statistics_gen.outputs.output, schema=infer_schema.outputs.output)\n",
    "\n",
    "  # Performs transformations and feature engineering in training and serving.\n",
    "  transform = Transform(\n",
    "      input_data=example_gen.outputs.examples,\n",
    "      schema=infer_schema.outputs.output,\n",
    "      module_file=module_file)\n",
    "\n",
    "  # Uses user-provided Python function that implements a model using\n",
    "  # TensorFlow's Estimators API.\n",
    "  trainer = Trainer(\n",
    "      custom_executor_spec=executor_spec.ExecutorClassSpec(\n",
    "          ai_platform_trainer_executor.Executor),\n",
    "      module_file=module_file,\n",
    "      transformed_examples=transform.outputs.transformed_examples,\n",
    "      schema=infer_schema.outputs.output,\n",
    "      transform_output=transform.outputs.transform_output,\n",
    "      train_args=trainer_pb2.TrainArgs(num_steps=10000),\n",
    "      eval_args=trainer_pb2.EvalArgs(num_steps=5000),\n",
    "      custom_config={'ai_platform_training_args': ai_platform_training_args})\n",
    "\n",
    "  # Uses TFMA to compute a evaluation statistics over features of a model.\n",
    "  model_analyzer = Evaluator(\n",
    "      examples=example_gen.outputs.examples,\n",
    "      model_exports=trainer.outputs.output,\n",
    "      feature_slicing_spec=evaluator_pb2.FeatureSlicingSpec(specs=[\n",
    "          evaluator_pb2.SingleSlicingSpec(\n",
    "              column_for_slicing=['weekday'])\n",
    "      ]))\n",
    "\n",
    "  # Performs quality validation of a candidate model (compared to a baseline).\n",
    "  model_validator = ModelValidator(\n",
    "      examples=example_gen.outputs.examples, model=trainer.outputs.output)\n",
    "\n",
    "  # Checks whether the model passed the validation steps and pushes the model\n",
    "  # to a file destination if check passed.\n",
    "  pusher = Pusher(\n",
    "      custom_executor_spec=executor_spec.ExecutorClassSpec(\n",
    "         ai_platform_pusher_executor.Executor),\n",
    "      model_export=trainer.outputs.output,\n",
    "      model_blessing=model_validator.outputs.blessing,\n",
    "      custom_config={'ai_platform_serving_args': ai_platform_serving_args})\n",
    "\n",
    "  return pipeline.Pipeline(\n",
    "      pipeline_name=pipeline_name,\n",
    "      pipeline_root=pipeline_root,\n",
    "      components=[\n",
    "          example_gen, statistics_gen, infer_schema, validate_stats, transform,\n",
    "          trainer, model_analyzer, model_validator, #pusher\n",
    "      ],\n",
    "      # enable_cache=True,\n",
    "      beam_pipeline_args=beam_pipeline_args\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile and deploy the pipeline\n",
    "You will now configure the GCP services utilized by the pipeline and then compile the pipeline to a YAML format utilized by Kubeflow Pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configure GCP services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GCP project and region to be used by AI Platform services\n",
    "_project_id = 'jk-tfw-demo'\n",
    "_gcp_region = 'us-central1'\n",
    "\n",
    "# Connection setting to Cloud SQL based ML Metadata\n",
    "_metadata_config = kubeflow_pb2.KubeflowMetadataConfig()\n",
    "_metadata_config.mysql_db_service_host.environment_variable = 'MYSQL_SERVICE_HOST'\n",
    "_metadata_config.mysql_db_service_port.environment_variable = 'MYSQL_SERVICE_PORT'\n",
    "_metadata_config.mysql_db_name.value = 'metadb'\n",
    "_metadata_config.mysql_db_user.value = 'root' \n",
    "_metadata_config.mysql_db_password.value = ''\n",
    "\n",
    "# GCS folder to be used to output the artifacts generated by the pipeline\n",
    "_pipeline_name = 'online_news_pipeline'\n",
    "_pipeline_root = 'gs://{}/{}/'.format(_artifact_store_bucket, _pipeline_name)\n",
    "\n",
    "# Dataflow settings\n",
    "_beam_tmp_folder = 'gs://{}/beam/tmp'.format(_artifact_store_bucket)\n",
    "_beam_pipeline_args = [\n",
    "    '--runner=DataflowRunner',\n",
    "    '--experiments=shuffle_mode=auto',\n",
    "    '--project=' + _project_id,\n",
    "    '--temp_location=' + _beam_tmp_folder,\n",
    "    '--region=' + _gcp_region,\n",
    "]\n",
    "\n",
    "# AI Platform Training settings\n",
    "# https://cloud.google.com/ml-engine/reference/rest/v1/projects.jobs#Job\n",
    "_ai_platform_training_args = {\n",
    "    'project': _project_id,\n",
    "    'region': _gcp_region\n",
    "}\n",
    "\n",
    "# AI Platform Prediction settings\n",
    "# https://cloud.google.com/ml-engine/reference/rest/v1/projects.models\n",
    "_model_name = 'online_news'\n",
    "_ai_platform_serving_args = {\n",
    "    'project_id': _project_id,\n",
    "    'model_name': _model_name\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compile the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the filename of the generated YAML\n",
    "_pipeline_file_name = '{}.yaml'.format(_pipeline_name)\n",
    "\n",
    "# Compile the pipeline\n",
    "runner_config = kubeflow_dag_runner.KubeflowDagRunnerConfig(\n",
    "    kubeflow_metadata_config=_metadata_config\n",
    ")\n",
    "kubeflow_dag_runner.KubeflowDagRunner(config=runner_config, output_filename=_pipeline_file_name).run(\n",
    "    _create_pipeline(\n",
    "        pipeline_name=_pipeline_name,\n",
    "        pipeline_root=_pipeline_root,\n",
    "        data_root = _gcs_data_root,\n",
    "        module_file=_gcs_module_file,\n",
    "        beam_pipeline_args=_beam_pipeline_args,\n",
    "        ai_platform_training_args=_ai_platform_training_args,\n",
    "        ai_platform_serving_args=_ai_platform_serving_args,\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upload the pipeline to Kubeflow Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get credentials to GKE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_cluster_name = 'tfw-demo-kfp-cluster'\n",
    "_cluster_zone = 'us-central1-a'\n",
    "\n",
    "!gcloud config set project $_project_id\n",
    "!gcloud container clusters get-credentials $_cluster_name --zone $_cluster_zone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_client = kfp.Client()\n",
    "_client.upload_pipeline(_pipeline_file_name, _pipeline_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "41nLEd9uW0UW"
   },
   "source": [
    "### Run the pipeline\n",
    "\n",
    "You can now trigger the run using the KFP UI or programmatically using `kfp.Client()`. "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "QNxEZK2pPA1P"
   ],
   "name": "TFX Lab 3 â€“ On-Prem with Beam Orchestrator",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "tfx",
   "language": "python",
   "name": "tfx"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
